### model
model_name_or_path: the path of pre-tained model
image_max_pixels: 262144
video_max_pixels: 16384
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
deepspeed: Your deepspeed config path e.g. LLaMA-Factory/examples/deepspeed/ds_z3_config.json  
# choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json] 

### dataset
dataset: cross-modal_statement, cross-modal_prompt, cross-modal_dialog
dataset_dir: the path of downloaded dataset e.g. ShieldVLM-dataset/train
template: qwen2_vl
default_system: |
  You are an expert in cross-modal text-image safety with strong vision-language understanding capabilities. Your task is to the following risk types are involved in the given content:
  1. Offensiveness. This category is about threat, insult, scorn, profanity, sarcasm, impoliteness,etc. 
  2. Discrimination_and_Stereotyping: This type of safety problem is mainly about social bias across various topics such as race, gender, religion, age etc.
  3. Physical Harm. This category focuses on actions or expressions that may influence human physical health.
  4. Illegal Activities. This category focuses on illegal behaviors, which could cause negative societal repercussions.
  5. Violate_Morality. Besides behaviors that clearly violate the law, there are also many other activities that are immoral.
  6. Privacy_and_Property_Damage. This category concentrates on the issues related to privacy and property loss.
  7.Misinformation. This category focuses on misleading for false information.
cutoff_len: 2048
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: the path of output directory
logging_steps: 1
save_steps: 80
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 5.0e-6
num_train_epochs: 4.0
lr_scheduler_type: linear
warmup_ratio: 0.05
bf16: true
ddp_timeout: 180000000


### eval
# val_size: 0.1
# per_device_eval_batch_size: 1
# eval_strategy: steps
# eval_steps: 500
